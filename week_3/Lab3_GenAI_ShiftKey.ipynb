{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPw+xfQu/DDc3pB/NIcJXJ2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shiftkey-labs/GenAI-Course/blob/main/week_3/Lab3_GenAI_ShiftKey.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ShiftKey GenAI Certification Lab3 - Fine Tuning Flan T5\n",
        "\n",
        "## Overview\n",
        "In this lab, we will guide you through the process of fine-tuning a pre-trained text generation model using the Hugging Face `transformers` library. The model we will use is Google's FLAN-T5, a state-of-the-art model for various sequence-to-sequence tasks such as text summarization. We will break down each step from loading the model and dataset to fine-tuning the model and using it to generate summaries. Finally, we will push the fine-tuned model to the Hugging Face Model Hub.\n",
        "\n",
        "## Table of Contents:\n",
        "1. Installing Necessary Libraries\n",
        "2. Loading the Model and Tokenizer\n",
        "3. Setting up the Device\n",
        "4. Loading the Dataset\n",
        "5. Splitting the Dataset\n",
        "6. Preprocessing the Dataset\n",
        "7. Tokenizing the Datasets\n",
        "8. Setting Training Arguments\n",
        "9. Creating the Trainer\n",
        "10. Training the Model\n",
        "11. Evaluating the Model\n",
        "12. Summarization Function\n",
        "13. Testing the Summarizer\n",
        "14. Testing the Model for\n",
        "15. Summarization Before Fine-Tuning\n",
        "\n",
        "\n",
        "## Installing Necessary Libraries\n",
        "We need to install the Hugging Face `transformers` and `datasets` libraries. These libraries provide pre-trained models and a variety of datasets that we can easily use for fine-tuning tasks."
      ],
      "metadata": {
        "id": "aYOd5pL9tQHJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xGa0q57dtO46"
      },
      "outputs": [],
      "source": [
        "!pip install transformers datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading the Model and Tokenizer\n",
        "In this section, we will load the FLAN-T5 small model, which is a sequence-to-sequence model designed for tasks like text summarization, translation, and other text generation tasks.\n",
        "\n",
        "* `AutoTokenizer` loads the tokenizer corresponding to the model. The tokenizer will convert raw text to input tokens the model can understand.\n",
        "\n",
        "* `AutoModelForSeq2SeqLM` loads a pre-trained model that is specifically designed for sequence-to-sequence tasks like summarization."
      ],
      "metadata": {
        "id": "vpovlEXd2v9o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "import torch\n",
        "\n",
        "# Define the model name and load the tokenizer and model\n",
        "model_name = \"google/flan-t5-small\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)"
      ],
      "metadata": {
        "id": "dAiaIQ5N2-FE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting up the Device\n",
        "Before we can use the model for training, we need to check whether a GPU is available. GPUs (Graphics Processing Units) are much faster than CPUs for model training tasks, so we should use one if possible.\n",
        "\n",
        "* We check if a CUDA-enabled GPU is available. If yes, we use the GPU (`cuda`), otherwise, we fall back to the CPU.\n",
        "\n",
        "* The model is then transferred to the appropriate device using `.to(device)`."
      ],
      "metadata": {
        "id": "y8KrcRXc3DnO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = model.to(device)\n",
        "\n",
        "# Print out which device we're using (GPU or CPU)\n",
        "print(device)"
      ],
      "metadata": {
        "id": "mLom68x03Gw8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing the Model for Summarization Before Fine-Tuning\n",
        "Before we fine-tune the model, let’s test how well the pre-trained model performs on a sample text. This will give us a baseline performance to compare with the fine-tuned model later."
      ],
      "metadata": {
        "id": "l0PArUX089Xk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def summarize(text):\n",
        "  inputs = tokenizer(text, return_tensors=\"pt\", max_length=512, truncation=True).to(device)\n",
        "  summary_ids = model.generate(inputs[\"input_ids\"], max_length=128, num_beams=4, early_stopping=True)\n",
        "  return tokenizer.decode(summary_ids[0], skip_special_tokens=True)"
      ],
      "metadata": {
        "id": "C0UwUj2693HI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a sample text for summarization\n",
        "sample_text =     \"\"\"\n",
        "Person A: Hey, did you hear about the new project management software our company is planning to implement?\n",
        "\n",
        "Person B: Yeah, I heard a bit about it. What’s the deal with it?\n",
        "\n",
        "Person A: It’s called \"TaskFlow.\" The management thinks it’s going to streamline our workflow, especially with remote teams. It’s supposed to integrate all the tools we use, like Slack, Trello, and Google Drive, into one platform.\n",
        "\n",
        "Person B: That sounds interesting. But I’m a bit concerned about the learning curve. Is it user-friendly?\n",
        "\n",
        "Person A: From what I’ve seen, it looks pretty intuitive. They’re also planning to run a couple of training sessions to get everyone up to speed. The first one is next Monday.\n",
        "\n",
        "Person B: Okay, that helps. I guess I’ll have to attend that session. How does it compare to what we’re using now?\n",
        "\n",
        "Person A: It’s supposed to be much more efficient. We’ll be able to track project progress more easily and get real-time updates. Plus, it has built-in analytics to help us with performance tracking.\n",
        "\n",
        "Person B: That sounds promising. I just hope it doesn’t come with too many bugs at launch.\n",
        "\n",
        "Person A: Yeah, that’s always a concern with new software. But they’ve been testing it for a while now, so fingers crossed it goes smoothly.\n",
        "\n",
        "Person B: Let’s hope for the best. Thanks for the info!\n",
        "\n",
        "Person A: No problem. See you at the training!\n",
        "\"\"\"\n",
        "\n",
        "# Summarize the sample text using the pre-trained model (without fine-tuning)\n",
        "pre_finetuned_summary = summarize(sample_text)\n",
        "print(\"Summary before fine-tuning:\", pre_finetuned_summary)"
      ],
      "metadata": {
        "id": "NkEa8vmo9F3p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading the Dataset\n",
        "Here, we load a popular dataset called `cnn_dailymail`, which is frequently used for summarization tasks. The dataset contains news articles paired with summaries, making it perfect for training a model to summarize text.\n",
        "\n",
        "The `load_dataset` function from the `datasets` library is used to load the `cnn_dailymail` dataset, specifying version `3.0.0`."
      ],
      "metadata": {
        "id": "cYJKf_ND3J9C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Load the CNN/DailyMail dataset, which contains articles and summaries\n",
        "dataset = load_dataset(\"cnn_dailymail\", \"3.0.0\", split=\"train\")"
      ],
      "metadata": {
        "id": "zLYP32_U3Mrx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Splitting the Dataset\n",
        "In this step, we split the dataset into training and evaluation sets. Training is done on one subset, while the other is used for evaluation (testing) to see how well the model generalizes to unseen data.\n",
        "\n",
        "* `train_test_split` divides the dataset into training and testing subsets.\n",
        "* We create a smaller training set for quick testing purposes."
      ],
      "metadata": {
        "id": "0IY30Rle3OXr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the dataset into training and testing subsets\n",
        "dataset_split = dataset.train_test_split(test_size=0.1)\n",
        "\n",
        "# Further reduce the training set size for faster testing during development\n",
        "small_train_dataset = dataset_split['train'].train_test_split(test_size=0.99)['train']\n",
        "eval_dataset = dataset_split['test']"
      ],
      "metadata": {
        "id": "SN3wAFL93lSh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing the Dataset\n",
        "Before feeding the data into the model, we need to tokenize it. This function handles preprocessing by tokenizing the input text (the news articles) and the target text (the summaries).\n",
        "\n",
        "* The `inputs` are tokenized and truncated to a maximum length of 512 tokens, which ensures they fit within the model’s input constraints.\n",
        "\n",
        "* The target (summary) is tokenized separately. In this case, we use a maximum length of 128 tokens for the target sequence.\n",
        "\n",
        "* The tokenized data is then moved to the appropriate device (GPU or CPU) for further processing."
      ],
      "metadata": {
        "id": "3JLcdIBO3oRq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_function(examples):\n",
        "  # Extract the articles from the dataset\n",
        "  inputs = [doc for doc in examples['article']]\n",
        "\n",
        "  # Tokenize the articles (inputs) with padding and truncation to a max length of 512\n",
        "  model_inputs = tokenizer(inputs, max_length=512, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
        "\n",
        "  # Tokenize the summaries (labels) using the target tokenizer context\n",
        "  with tokenizer.as_target_tokenizer():\n",
        "    labels = tokenizer(examples['highlights'], max_length=128, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
        "\n",
        "  # Attach the tokenized summaries as labels to the model inputs\n",
        "  model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "\n",
        "  # Move the tokenized inputs and labels to the appropriate device (GPU/CPU)\n",
        "  model_inputs = {k: v.to(device) for k, v in model_inputs.items()}\n",
        "\n",
        "  return model_inputs"
      ],
      "metadata": {
        "id": "rGBH0IW93qQS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenizing the Datasets\n",
        "We apply the preprocessing function to both the training and evaluation datasets. The `.map()` method applies the function to every element in the dataset.\n",
        "\n",
        "Tokenization is done in batches to speed up the process."
      ],
      "metadata": {
        "id": "KntOdXjb46Q6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize the small training dataset\n",
        "tokenized_train_dataset = small_train_dataset.map(preprocess_function, batched=True)\n",
        "\n",
        "# Tokenize the evaluation dataset\n",
        "tokenized_eval_dataset = eval_dataset.map(preprocess_function, batched=True)"
      ],
      "metadata": {
        "id": "I7FADB7W4_eE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting Training Arguments\n",
        "The next step is to define the training parameters. We use the `Seq2SeqTrainingArguments` class from the `transformers` library to specify key settings such as learning rate, batch size, and the number of epochs.\n",
        "\n",
        "* `evaluation_strategy=\"epoch\"` ensures the model is evaluated after every epoch.\n",
        "\n",
        "* `num_train_epochs=3` sets the number of times the model will go through the entire training dataset.\n",
        "\n",
        "* `predict_with_generate=True` ensures the model generates summaries during evaluation."
      ],
      "metadata": {
        "id": "I14tlqw-5OoT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Seq2SeqTrainingArguments\n",
        "\n",
        "# Define training arguments\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir='./results',              # Directory to save the model checkpoints\n",
        "    evaluation_strategy=\"epoch\",         # Evaluate the model at the end of every epoch\n",
        "    learning_rate=2e-5,                  # Learning rate for the optimizer\n",
        "    per_device_train_batch_size=8,       # Batch size for training\n",
        "    per_device_eval_batch_size=8,        # Batch size for evaluation\n",
        "    weight_decay=0.01,                   # Regularization to prevent overfitting\n",
        "    save_total_limit=3,                  # Only keep the last 3 checkpoints\n",
        "    num_train_epochs=3,                  # Number of training epochs\n",
        "    predict_with_generate=True,          # Enable text generation during evaluation\n",
        "    logging_dir=\"./logs\"                 # Directory for storing training logs\n",
        ")"
      ],
      "metadata": {
        "id": "JqVonjlJ5xfe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating the Trainer\n",
        "The `Seq2SeqTrainer` class handles the entire training and evaluation loop. We provide it with the model, the training arguments, and the datasets.\n",
        "\n",
        "This class simplifies the training process and abstracts away the complexities of manually writing the training loop."
      ],
      "metadata": {
        "id": "BrQuPwox6QQt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Seq2SeqTrainer\n",
        "\n",
        "# Create the trainer object\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,                         # The model to be trained\n",
        "    args=training_args,                  # The training arguments defined earlier\n",
        "    train_dataset=tokenized_train_dataset,  # The tokenized training dataset\n",
        "    eval_dataset=tokenized_eval_dataset,    # The tokenized evaluation dataset\n",
        "    tokenizer=tokenizer                  # The tokenizer to handle input and output\n",
        ")"
      ],
      "metadata": {
        "id": "4FB7tuyi6UCh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training the Model\n",
        "Now, we start the training process. The trainer will iterate over the training data for the specified number of epochs.\n",
        "\n",
        "This is the step where the model learns from the data and fine-tunes itself based on the task."
      ],
      "metadata": {
        "id": "1QGCvkRP6ZKI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's train\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "fP00IFjb6b76"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluating the Model\n",
        "After the training is complete, we evaluate the model on the evaluation dataset. This step measures how well the model generalizes to unseen data.\n",
        "\n",
        "The trainer returns metrics such as accuracy and loss, which give us an idea of the model’s performance."
      ],
      "metadata": {
        "id": "dEJ-aMyF6pW9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model on the evaluation dataset\n",
        "metrics = trainer.evaluate()\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(metrics)"
      ],
      "metadata": {
        "id": "2Rk-oMbY6u6e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Summarization Function\n",
        "To use the fine-tuned model, we define a function that takes in text and generates a summary. This function tokenizes the input text and uses the `model.generate()` function to create the summary.\n",
        "\n",
        "`num_beams=4` specifies the number of beams used for beam search, which helps in generating better summaries by exploring multiple possibilities."
      ],
      "metadata": {
        "id": "b-iZscte60eg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def summarize(text):\n",
        "  # Tokenize the input text and move it to the correct device\n",
        "  inputs = tokenizer(text, return_tensors=\"pt\", max_length=512, truncation=True).to(device)\n",
        "\n",
        "  # Generate the summary using the fine-tuned model\n",
        "  summary_ids = model.generate(inputs[\"input_ids\"], max_length=128, num_beams=4, early_stopping=True)\n",
        "\n",
        "  # Decode the generated summary back into text and return it\n",
        "  return tokenizer.decode(summary_ids[0], skip_special_tokens=True)"
      ],
      "metadata": {
        "id": "gPmWE_dw60E5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing the Summarizer\n",
        "Finally, we test the summarizer function with sample texts to see how well the fine-tuned model performs in real-world scenarios."
      ],
      "metadata": {
        "id": "y529LT056_-S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(summarize(\n",
        "    \"\"\"\n",
        "Person A: Hey, did you hear about the new project management software our company is planning to implement?\n",
        "\n",
        "Person B: Yeah, I heard a bit about it. What’s the deal with it?\n",
        "\n",
        "Person A: It’s called \"TaskFlow.\" The management thinks it’s going to streamline our workflow, especially with remote teams. It’s supposed to integrate all the tools we use, like Slack, Trello, and Google Drive, into one platform.\n",
        "\n",
        "Person B: That sounds interesting. But I’m a bit concerned about the learning curve. Is it user-friendly?\n",
        "\n",
        "Person A: From what I’ve seen, it looks pretty intuitive. They’re also planning to run a couple of training sessions to get everyone up to speed. The first one is next Monday.\n",
        "\n",
        "Person B: Okay, that helps. I guess I’ll have to attend that session. How does it compare to what we’re using now?\n",
        "\n",
        "Person A: It’s supposed to be much more efficient. We’ll be able to track project progress more easily and get real-time updates. Plus, it has built-in analytics to help us with performance tracking.\n",
        "\n",
        "Person B: That sounds promising. I just hope it doesn’t come with too many bugs at launch.\n",
        "\n",
        "Person A: Yeah, that’s always a concern with new software. But they’ve been testing it for a while now, so fingers crossed it goes smoothly.\n",
        "\n",
        "Person B: Let’s hope for the best. Thanks for the info!\n",
        "\n",
        "Person A: No problem. See you at the training!\n",
        "\"\"\"\n",
        "))"
      ],
      "metadata": {
        "id": "hBLRVAnv69su"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}